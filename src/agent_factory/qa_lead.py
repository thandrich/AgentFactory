import logging
    """
    The QA Lead agent is responsible for validating the Engineer's work.
    It autonomously generates test cases based on the agent's goal, 
    executes the code, and grades the performance.
    """

    def __init__(self, model_name: str = "gemini-2.5-flash"):
        self.model_name = model_name
        self.model_config = Gemini(model=model_name)
        
        # We use a generic 'Thinker' agent configuration that we will 
        # re-prompt for different stages (Test Generation vs Evaluation).
        self.llm_instruction = """
        You are The QA Lead, a strict and adversarial software tester.
        You do not accept mediocrity. You verify that code works exactly as intended.
        """
        
        self.agent = LlmAgent(
            name="QA_Lead",
            model=self.model_config,
            instruction=self.llm_instruction
        )
        self.runner = InMemoryRunner(agent=self.agent)
        logger.info(f"QA Lead initialized with model: {model_name}")

    def review_agent(self, agent_code: str, agent_definition: Dict[str, Any]) -> Dict[str, Any]:
        """
        Main entry point. Generates a test, runs the code, and evaluates the output.
        
        Args:
            agent_code: The Python source code string generated by the Engineer.
            agent_definition: The JSON object defining what the agent IS SUPPOSED to do.
        """
        logger.info(f"QA Lead starting review for agent: {agent_definition.get('agent_name', 'Unknown')}")

        # Step 1: Generate a Test Case
        test_case = self._generate_test_scenario(agent_definition)
        if not test_case:
            return {"status": "FAIL", "reason": "Failed to generate test case."}

        logger.info(f"Generated Test Input: {test_case['input']}")

        # Step 2: Execute the Agent
        execution_result = self._execute_agent(agent_code, test_case['input'])
        
        if not execution_result['success']:
            # If code crashed, immediate fail
            return {
                "status": "FAIL",
                "score": 0,
                "reasoning": "The agent code raised an exception during execution.",
                "execution_logs": execution_result.get('error', 'Unknown error'),
                "test_input_used": test_case['input']
            }

        # Step 3: Evaluate the Output
        evaluation = self._evaluate_output(
            agent_definition, 
            test_case, 
            execution_result['output']
        )
        
        # Combine results
        return {
            "status": "PASS" if evaluation['passed'] else "FAIL",
            "score": evaluation['score'],
            "reasoning": evaluation['reasoning'],
            "feedback": evaluation['suggestions'],
            "test_context": {
                "input": test_case['input'],
                "expected_behavior": test_case['expected_behavior'],
                "actual_output": execution_result['output']
            }
        }

    def _generate_test_scenario(self, agent_definition: Dict[str, Any]) -> Optional[Dict[str, str]]:
        """
        Analyzes the agent definition and invents a specific input to test it.
        """
        prompt = f"""
        **TASK**: Create a test case for the following AI Agent.
        
        **AGENT DEFINITION**:
        {json.dumps(agent_definition, indent=2)}
        
        **INSTRUCTIONS**:
        1. Analyze the inputs and goals.
        2. Create a specific, challenging input (prompt or data) that tests the agent's core functionality.
        3. Define what the success criteria looks like.
        
        **OUTPUT STRICT JSON**:
        {{
            "input": "The actual string/data to feed the agent",
            "expected_behavior": "Description of what the result should be"
        }}
        """
        
        response = self._run_llm(prompt)
        try:
            return json.loads(self._clean_json(response))
        except Exception as e:
            logger.error(f"Failed to generate test case: {e}")
            return None

    def _execute_agent(self, code: str, test_input: str) -> Dict[str, Any]:
        """
        Dynamically executes the provided agent code string in a safe-ish sandbox.
        """
        try:
            # Create a dedicated local scope
            local_scope = {}
            
            # Execute the string definition to load the class/agent object
            exec(code, {}, local_scope)
            
            # The Engineer contract states the agent object is named 'agent'
            if 'agent' not in local_scope:
                return {"success": False, "error": "Agent object 'agent' not found in code."}
            
            target_agent = local_scope['agent']
            
            # Run the agent using an in-memory runner
            async def _run_target():
                target_runner = InMemoryRunner(agent=target_agent)
                # We use run_debug to capture events
                events = await target_runner.run_debug(test_input)
                
                # Extract the final text response
                final_text = ""
                for event in reversed(events):
                    if hasattr(event, 'content') and event.content and event.content.parts:
                        for part in event.content.parts:
                            if part.text:
                                final_text = part.text
                                return final_text # Return first found content from end
                return final_text

            output = asyncio.run(_run_target())
            return {"success": True, "output": output}

        except Exception as e:
            logger.error(f"Runtime error during agent testing: {e}")
            return {"success": False, "error": traceback.format_exc()}

    def _evaluate_output(self, agent_def: Dict, test_case: Dict, actual_output: str) -> Dict[str, Any]:
        """
        Compare Actual vs Expected using the LLM.
        """
        prompt = f"""
        **JUDGEMENT TIME**
        
        **Agent Goal**: {agent_def.get('goal')}
        **Test Input**: {test_case['input']}
        **Expected Behavior**: {test_case['expected_behavior']}
        
        **ACTUAL OUTPUT GENERATED**:
        {actual_output}
        
        **TASK**:
        Evaluate the Actual Output against the Expected Behavior.
        
        **CRITERIA**:
        1. Accuracy: Did it answer the request?
        2. Format: Did it follow any strict output formats (like JSON) if requested?
        3. Tools: Did it appear to use tools if necessary?
        
        **OUTPUT STRICT JSON**:
        {{
            "passed": true/false,
            "score": (integer 1-10),
            "reasoning": "Brief explanation",
            "suggestions": ["List of fix suggestions for the Engineer"]
        }}
        """
        
        response = self._run_llm(prompt)
        try:
            return json.loads(self._clean_json(response))
        except Exception:
            # Fallback if JSON fails
            return {
                "passed": False, 
                "score": 0, 
                "reasoning": "QA Judge failed to parse its own evaluation.",
                "suggestions": []
            }

    def _run_llm(self, prompt: str) -> str:
        """Helper to run the internal LLM logic."""
        async def _r():
            events = await self.runner.run_debug(prompt)
            for event in reversed(events):
                if hasattr(event, 'content') and event.content and event.content.parts:
                    for part in event.content.parts:
                        if part.text:
                            return part.text
            return ""
        return asyncio.run(_r())

    def _clean_json(self, text: str) -> str:
        """Helper to clean markdown code blocks."""
        cleaned = text.strip()
        if cleaned.startswith("```json"):
            cleaned = cleaned[7:]
        elif cleaned.startswith("```"):
            cleaned = cleaned[3:]
        if cleaned.endswith("```"):
            cleaned = cleaned[:-3]
        return cleaned.strip()